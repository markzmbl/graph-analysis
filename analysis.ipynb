{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import pickle\n",
    "from bisect import bisect_right\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bitarray.util import zeros\n",
    "from intervaltree import IntervalTree\n",
    "from matplotlib import pyplot as plt\n",
    "from networkx.readwrite.json_graph.node_link import node_link_graph\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dscent.time_sequence import trim_before, trim_after\n",
    "from input.iterator import GraphEdgeIterator"
   ],
   "id": "711eca68b377bdbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "raw_df = pd.read_csv(\"may-run-omega0050_cycles.csv\", delimiter=\";\")\n",
    "raw_df[\"candidates\"] = raw_df[\"candidates\"].apply(lambda x: set(json.loads(x)))\n",
    "raw_df[\"bundled_cycle\"] = raw_df[\"bundled_cycle\"].apply(lambda x: (node_link_graph(json.loads(x), edges=\"edges\")))\n",
    "raw_df[\"vertices\"] = raw_df[\"bundled_cycle\"].apply(lambda x: set(x.nodes()))\n",
    "for index, cycle in tqdm(raw_df[\"bundled_cycle\"].items(), total=len(raw_df), desc=\"Processing cycles\", unit_scale=True):\n",
    "    first, *_, last = sorted(cycle.edges(keys=True), key=lambda x: x[2])\n",
    "    raw_df.loc[index, \"begin\"] = first[2]\n",
    "    raw_df.loc[index, \"end\"] = last[2]\n",
    "raw_df[[\"begin\", \"end\"]] = raw_df[[\"begin\", \"end\"]].astype(int)"
   ],
   "id": "71cb240601a43a4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build interval tree with row index as data\n",
    "intervals = IntervalTree()\n",
    "for idx, row in tqdm(raw_df.iterrows(), total=len(raw_df), desc=\"Building intervals\", unit_scale=True):\n",
    "    intervals[row[\"begin\"]: row[\"end\"] + 1] = idx  # End is inclusive"
   ],
   "id": "f69552a4bf339764"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chunk_size = 10_000\n",
    "sorted_keys, sorted_values = zip(*intervals.boundary_table.items())\n",
    "\n",
    "aggregated_keys = []\n",
    "aggregated_values = []\n",
    "\n",
    "for i in range(0, len(sorted_keys), chunk_size):\n",
    "    chunk_keys = sorted_keys[i:i + chunk_size]\n",
    "    chunk_values = sorted_values[i:i + chunk_size]\n",
    "\n",
    "    # Use the midpoint key as a representative key\n",
    "    aggregated_keys.append(np.mean(chunk_keys))\n",
    "    aggregated_values.append(sum(chunk_values))\n",
    "\n",
    "# Plotting\n",
    "plt.plot(aggregated_keys, aggregated_values)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Mean key of chunk\")\n",
    "plt.ylabel(\"Sum of values in chunk\")\n",
    "plt.title(\"Aggregated Boundary Table (100-key bins)\")\n",
    "plt.show()"
   ],
   "id": "b14862cdb08d9ada"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    # Step 1: Create a mapping from vertex to bit position\n",
    "all_vertices = set.union(*raw_df[\"vertices\"])\n",
    "vertex_to_index = {v: i for i, v in enumerate(sorted(all_vertices))}\n",
    "num_bits = len(vertex_to_index)\n",
    "\n",
    "# Step 2: Convert each vertex set to a bitarray\n",
    "def set_to_bitarray(vertex_set):\n",
    "    ba = zeros(num_bits)\n",
    "    for v in vertex_set:\n",
    "        ba[vertex_to_index[v]] = 1\n",
    "    return ba\n",
    "\n",
    "vertices_bitarrays = []\n",
    "for i, vertices in tqdm(raw_df[\"vertices\"].items(), total=len(raw_df), unit_scale=True):\n",
    "    vertices_bitarrays.append(set_to_bitarray(vertices))"
   ],
   "id": "73e568a85d7387d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 3: Proceed with your logic using bitarrays\n",
    "edge_list = []\n",
    "sorted_df = raw_df.sort_values([\"begin\", \"end\"], ascending=[True, False])\n",
    "grouped_by_begin = sorted_df.groupby(\"begin\")\n",
    "\n",
    "pbar = tqdm(total=len(raw_df[[\"begin\", \"end\"]].drop_duplicates()), desc=\"Building meta graph\", unit_scale=True, smoothing=1)\n",
    "\n",
    "for begin_i, begin_group in grouped_by_begin:\n",
    "    max_end = begin_group[\"end\"].iloc[0]\n",
    "\n",
    "    current_intervals = sorted(\n",
    "        intervals[begin_i: max_end + 1],\n",
    "        key=lambda iv: (iv.begin, -iv.end)\n",
    "    )\n",
    "    slice_index = len(current_intervals)\n",
    "\n",
    "    for end_i, group in begin_group.groupby(\"end\"):\n",
    "        new_slice_index = bisect_right([iv.begin for iv in current_intervals], end_i)\n",
    "        if new_slice_index < slice_index:\n",
    "            slice_index = new_slice_index\n",
    "            current_intervals = current_intervals[: new_slice_index]\n",
    "\n",
    "        overlap_indices = {iv.data for iv in current_intervals}\n",
    "\n",
    "        for i in group.index:\n",
    "            vertices_bitarray_i = vertices_bitarrays[i]\n",
    "            for j in overlap_indices:\n",
    "                if j > i and (vertices_bitarray_i & vertices_bitarrays[j]).any():\n",
    "                    edge_list.append((i, j))\n",
    "        pbar.update(1)\n",
    "\n",
    "meta_graph = nx.Graph()\n",
    "meta_graph.add_edges_from(edge_list)"
   ],
   "id": "a413a130d1e19f6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(edge_list), num_bits ** 2",
   "id": "f7d8343704021aa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_index(address_string):\n",
    "    for chunk in pd.read_csv(\"input/node_ids.txt\", header=None, chunksize=10_000):\n",
    "        for eth_index, eth_address in chunk.iloc[:, 0].items():\n",
    "            if eth_address == address_string:\n",
    "                return eth_index"
   ],
   "id": "2afb97caaa6a8ad3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "meebits_string = \"7bd29408f11d2bfc23c34f18275bbf23bb716bc7\"\n",
    "meebits_index = get_index(meebits_string)\n",
    "meebits_bytes = bytes.fromhex(meebits_string)"
   ],
   "id": "d6b702939c530b24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "meebits_edges_file = Path(\"meebits_edges.pickle\")\n",
    "if meebits_edges_file.exists():\n",
    "    with open(meebits_edges_file, \"rb\") as f:\n",
    "        meebits_edges = pickle.load(f)\n",
    "else:\n",
    "    meebits_edges = [\n",
    "        (u, v, t, data)\n",
    "        for u, v, t, data\n",
    "        in tqdm(GraphEdgeIterator(\"2021-10-01\", \"2022-07-01\"), unit_scale=True)\n",
    "        if meebits_index in (u, v)\n",
    "    ]\n",
    "    with meebits_edges_file.open(\"wb\") as f:\n",
    "        pickle.dump(meebits_edges, f)\n",
    "\n",
    "meebits_star_graph = nx.MultiDiGraph()\n",
    "meebits_star_graph.add_edges_from(meebits_edges)\n",
    "print(meebits_star_graph)\n",
    "print(\"In-Degree of meebits:\", meebits_star_graph.in_degree(meebits_index))\n",
    "print(\"Out-Degree of meebits:\", meebits_star_graph.out_degree(meebits_index))"
   ],
   "id": "3654add49cc73f52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_dwell_times(graph: nx.MultiDiGraph):\n",
    "    # Initialize nested defaultdict to store dwell times.\n",
    "    dwell_times = defaultdict(lambda: defaultdict(list))\n",
    "    # Iterate over all nodes in the graph, treating each as a potential sender\n",
    "    for sender in graph.nodes():\n",
    "        # Look at all direct connections from this sender\n",
    "        for intermediate, incoming_transactions in graph[sender].items():\n",
    "            # Skip self-loops\n",
    "            if sender == intermediate:\n",
    "                continue\n",
    "            # Sort the timestamps of incoming transactions to the intermediate node\n",
    "            incoming_transactions = sorted(incoming_transactions.keys())\n",
    "            # Pair each incoming transaction with the next one to form time windows\n",
    "            incoming_transaction_pairs = zip(incoming_transactions, incoming_transactions[1:] + [None])\n",
    "            for incoming_timestamp, next_incoming_time in incoming_transaction_pairs:\n",
    "                # Examine all outgoing meebits_edges from the intermediate node\n",
    "                for recipient, outgoing_transactions in graph[intermediate].items():\n",
    "                    # Skip self-loops again\n",
    "                    if intermediate == recipient:\n",
    "                        continue\n",
    "                    # Sort the timestamps of outgoing transactions from intermediate to recipient\n",
    "                    outgoing_timestamps = sorted(outgoing_transactions.keys())\n",
    "                    # Remove any outgoing timestamps that are before the incoming timestamp (strictly)\n",
    "                    trim_before(outgoing_timestamps, lower_limit=incoming_timestamp, strict=True)\n",
    "                    # If there is a subsequent incoming transaction, trim away timestamps after it (non-include_limit)\n",
    "                    if next_incoming_time is not None:\n",
    "                        trim_after(outgoing_timestamps, upper_limit=next_incoming_time, strict=False)\n",
    "                    # For valid outgoing timestamps, calculate dwell time and add to result set\n",
    "                    for outgoing_timestamp in outgoing_timestamps:\n",
    "                        dwell_times[intermediate][sender].append(outgoing_timestamp - incoming_timestamp)\n",
    "    return dwell_times\n",
    "\n",
    "\n",
    "def get_burstiness(graph: nx.MultiDiGraph):\n",
    "    dwell_times = get_dwell_times(graph)\n",
    "    if len(dwell_times) == 0:\n",
    "        return np.nan\n",
    "    dwell_durations = []\n",
    "    for intermediate, senders in dwell_times.items():\n",
    "        for sender, durations in senders.items():\n",
    "            # dwell_durations.extend(durations)\n",
    "            dwell_durations.append(min(durations))\n",
    "            # dwell_durations.append(np.mean(list(durations)))\n",
    "\n",
    "    sigma, mu = np.std(dwell_durations), np.mean(dwell_durations)\n",
    "    if sigma == mu:\n",
    "        return 0.0\n",
    "    return (sigma - mu) / (sigma + mu)\n",
    "\n",
    "\n",
    "def draw_graph(graph):\n",
    "    # Define the layout\n",
    "    pos = nx.spring_layout(graph)\n",
    "\n",
    "    # Draw nodes and labels\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=500)\n",
    "    nx.draw_networkx_labels(graph, pos)\n",
    "\n",
    "    # Draw each edge with a different curvature\n",
    "    edges = list(graph.edges(keys=True))\n",
    "    for i, (u, v, k) in enumerate(edges):\n",
    "        nx.draw_networkx_edges(\n",
    "            graph,\n",
    "            pos,\n",
    "            edgelist=[(u, v)],\n",
    "            connectionstyle=f'arc3,rad={(i - len(edges) / 2) * 0.2}',\n",
    "            edge_color='black'\n",
    "        )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "id": "e1bf3483a836e9fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "48d3407f2dbc1243"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "component = components[3]\n",
    "len(component)\n",
    "# Sort the component_df\n",
    "component_df = (\n",
    "    raw_df.loc[list(component)].sort_values(\n",
    "        by=[\"seed_begin\", \"seed_end\"],\n",
    "        ascending=[True, False]\n",
    "    )\n",
    ")\n",
    "for bundled_cycle in component_df[\"bundled_cycle\"]:\n",
    "    # draw_graph(bundled_cycle)\n",
    "    pass\n",
    "plt.hist([graph.number_of_nodes() for graph in component_df[\"bundled_cycle\"]])\n",
    "plt.show()\n",
    "plt.hist([graph.number_of_edges() for graph in component_df[\"bundled_cycle\"]])\n",
    "plt.show()"
   ],
   "id": "91cadcb94f45090"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "raw_df[\"meebits\"] = False\n",
    "for u, v, t, data in tqdm(GraphEdgeIterator(\"2021-10-01\", \"2022-07-01\"), unit_scale=True):\n",
    "    if\n",
    "        open_df = raw_df[~raw_df[\"meebits\"]]\n",
    "    open_df[\"meebits\"] = open_df[\"vertices\"].apply(lambda x: meebits_index in x)\n",
    "\n",
    "    is_to_edge = u in merged_graph.nodes() and meebits_bytes == data[\"target_id\"]\n",
    "    is_from_edge = meebits_bytes == data[\"source_id\"] and v in merged_graph.nodes()\n",
    "    if is_to_edge or is_from_edge:\n",
    "        merged_graph.add_edge(u, v, key=t, **data)\n",
    "        meebits_edges.append((u, v, t, data))\n",
    "\n",
    "print(\"Number of meebits_edges:\", len(meebits_edges))\n",
    "\n",
    "# Find the SCC that contains the target_node\n",
    "for component in nx.strongly_connected_components(merged_graph):\n",
    "    if meebits_index in component:\n",
    "        print(\"Strongly Connected Component containing node\", me, \":\", component)\n",
    "        break"
   ],
   "id": "f5604a3e49d95d0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inside = 0\n",
    "outside = 0\n",
    "\n",
    "meebits_component = {meebits_index}\n",
    "for component in nx.strongly_connected_components(merged_graph):\n",
    "    if any(merged_graph.has_edge(component_vertex, meebits_index) for component_vertex in component):\n",
    "        meebits_component |= component\n",
    "print(\"Size of meebits_component:\", len(meebits_component))"
   ],
   "id": "d749cbc2c96fad8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c331a1257425fda5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "burstiness = get_burstiness(merged_graph.subgraph(meebits_component))\n",
    "print(f\"Burstiness of meebits component: {burstiness:.4f}\")"
   ],
   "id": "a70b512de6e51d91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "raw_df[\"burstiness\"] = raw_df[\"bundled_cycle\"].apply(get_burstiness)\n",
    "burstiness = raw_df[\"burstiness\"].dropna()\n",
    "burstiness = burstiness[burstiness > -1]\n",
    "plt.violinplot(burstiness, showmeans=True, showmedians=True)\n",
    "plt.show()"
   ],
   "id": "2cfb5c91047b454e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53aa6097cf831a38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f6225f273046f552"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
